{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b787a81f",
   "metadata": {},
   "source": [
    "# **Introduction au Modern Data Stack**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c531f9",
   "metadata": {},
   "source": [
    "Nous vivons une époque où les données sont devenues l'or noir du 21ème siècle. Chaque jour, des milliards d'interactions génèrent des volumes de données astronomiques : achats en ligne, posts sur les réseaux sociaux, capteurs connectés, transactions bancaires, visionnages de vidéos. Mais collecter des données ne suffit pas. Le véritable défi est de les transformer en informations exploitables pour prendre de meilleures décisions. C'est précisément là qu'intervient le Modern Data Stack.\n",
    "\n",
    "Le terme \"Modern Data Stack\" désigne l'ensemble des technologies, architectures et pratiques actuelles pour collecter, stocker, transformer et analyser les données. Le mot \"moderne\" n'est pas anodin : il marque une rupture avec les approches traditionnelles, héritées des années 1990-2000, qui ne sont plus adaptées aux enjeux actuels. Ce chapitre vous propose de comprendre pourquoi nous avons besoin d'un Modern Data Stack, comment nous en sommes arrivés là, et quels sont les acteurs majeurs de cet écosystème."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc0388d",
   "metadata": {},
   "source": [
    "## **1. Pourquoi le Modern Data Stack ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768d2dcc",
   "metadata": {},
   "source": [
    "### **1.1 Les limites des approches traditionnelles**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1de72de",
   "metadata": {},
   "source": [
    "Pour comprendre pourquoi le Modern Data Stack s'est imposé, il faut d'abord comprendre les limites des systèmes traditionnels. Dans les années 1990-2000, la plupart des entreprises utilisaient des systèmes dits \"on-premise\", c'est-à-dire hébergés dans leurs propres locaux, avec des serveurs physiques installés dans des salles informatiques climatisées.\n",
    "\n",
    "Imaginez une grande banque au début des années 2000. Elle possède plusieurs dizaines de serveurs pour héberger son Data Warehouse, construit avec des technologies propriétaires comme Oracle ou Teradata. L'investissement initial est colossal : achat des serveurs, installation, climatisation, électricité, équipes techniques dédiées. Et si la banque a besoin de plus de puissance de calcul pour traiter davantage de données ? Il faut commander de nouveaux serveurs, attendre leur livraison (plusieurs semaines), les installer, les configurer. Ce processus peut prendre des mois.\n",
    "\n",
    "De plus, ces systèmes traditionnels étaient conçus pour un type de données bien précis : les données structurées, c'est-à-dire des tableaux avec des lignes et des colonnes. Or, dès les années 2010, l'explosion du web, des réseaux sociaux et des objets connectés a généré une diversité de données que ces systèmes ne pouvaient pas gérer : vidéos, images, logs JSON, fichiers texte non structurés, flux temps réel.\n",
    "\n",
    "Enfin, le coût était prohibitif. Une licence Teradata ou Oracle pouvait coûter des centaines de milliers, voire des millions d'euros par an. Seules les très grandes entreprises pouvaient se permettre de tels investissements. Les PME et startups n'avaient tout simplement pas accès à ces technologies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc79605b",
   "metadata": {},
   "source": [
    "### **1.2 Les promesses du Modern Data Stack**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c14e40",
   "metadata": {},
   "source": [
    "Le Modern Data Stack répond à ces limitations en s'appuyant sur quatre piliers fondamentaux.\n",
    "\n",
    "**Le Cloud computing** constitue le premier pilier. Au lieu de posséder et maintenir ses propres serveurs, une entreprise peut louer de la puissance de calcul et du stockage auprès de fournisseurs comme AWS, Azure ou Google Cloud. L'avantage est immédiat : plus besoin d'investissement initial massif, on ne paie que ce qu'on utilise. Et surtout, l'élasticité est totale : besoin de traiter 10 fois plus de données ce mois-ci ? On augmente la puissance en quelques clics, puis on la réduit le mois suivant. Cette flexibilité était impensable avec les systèmes on-premise.\n",
    "\n",
    "**La séparation stockage-calcul** est le deuxième pilier. Traditionnellement, stockage et puissance de calcul étaient liés : pour avoir plus de capacité de traitement, il fallait acheter plus de serveurs avec plus de disques durs. Le Modern Data Stack découple ces deux aspects. On peut stocker des pétaoctets de données à très faible coût dans des solutions comme AWS S3, et ne mobiliser de la puissance de calcul que lorsqu'on en a besoin pour analyser ces données. C'est comme avoir une immense bibliothèque (stockage) et pouvoir embaucher des équipes de lecture (calcul) uniquement quand on veut consulter des livres.\n",
    "\n",
    "**L'ouverture et l'interopérabilité** constituent le troisième pilier. Les systèmes traditionnels étaient souvent des \"boîtes noires\" propriétaires où l'on était captif d'un fournisseur unique. Le Modern Data Stack privilégie les formats ouverts (Parquet, Delta, Iceberg), les API standards, et la possibilité de faire communiquer différents outils entre eux. Vous pouvez utiliser dbt pour transformer vos données, Airflow pour orchestrer, Looker pour visualiser, et tout cela fonctionne ensemble de manière fluide.\n",
    "\n",
    "**L'accessibilité démocratisée** est le quatrième pilier. Grâce au Cloud et aux modèles de tarification à l'usage, même une startup de trois personnes peut utiliser les mêmes technologies que Netflix ou Spotify. Il n'y a plus de barrière financière à l'entrée. Cette démocratisation a accéléré l'innovation et permis à des milliers d'entreprises de devenir data-driven.\n",
    "\n",
    "\n",
    "| Dimension | Legacy Data Stack | Modern Data Stack |\n",
    "|---------|------------------|------------------|\n",
    "| **Infrastructure** | Basée sur des serveurs on-premise, nécessitant des investissements matériels importants et une maintenance continue. La montée en charge est limitée par les ressources physiques disponibles. | Infrastructure cloud-native (AWS, GCP, Azure) offrant scalabilité, élasticité et réduction des coûts initiaux. Les équipes peuvent augmenter ou réduire les ressources à la demande. |\n",
    "| **Ingestion des données** | Reposait principalement sur du traitement batch avec des planifications fixes. Les sources étaient limitées aux systèmes internes, avec des intégrations complexes pour en ajouter de nouvelles. | Supporte à la fois le streaming temps réel (ex. Apache Kafka) et l’ingestion batch. Des outils modernes comme Airbyte ou Fivetran s’intègrent facilement aux APIs, plateformes SaaS et objets connectés. |\n",
    "| **Stockage** | Utilisation de bases de données relationnelles rigides et coûteuses, avec une scalabilité et une flexibilité limitées. | Utilise des data warehouses cloud (ex. Snowflake, BigQuery) et des data lakes permettant un stockage élastique, le support des données structurées et non structurées, et de hautes performances de requêtage. |\n",
    "| **Traitement** | Les pipelines ETL impliquaient souvent du scripting manuel et manquaient de modularité, entraînant des temps de traitement lents et un taux d’erreurs plus élevé. | Les pipelines ELT s’appuient sur des outils comme dbt pour des transformations modulaires et déclaratives. Le traitement est automatisé, plus rapide et plus facile à déboguer. |\n",
    "| **Flexibilité et intégration** | Les systèmes propriétaires entraînaient un fort verrouillage fournisseur, rendant les intégrations complexes et coûteuses. | Les outils open-source et orientés API garantissent interopérabilité et flexibilité, permettant de composer des stacks personnalisées sans dépendre d’un seul fournisseur. |\n",
    "| **Modèle de coûts** | Fort investissement initial (CapEx) pour le matériel et les licences logicielles. | Modèle de paiement à l’usage (pay-as-you-go) réduisant les barrières financières et alignant les coûts sur la consommation réelle. |\n",
    "| **Accessibilité** | Accès centralisé géré par les équipes IT, créant des goulets d’étranglement et limitant la démocratisation de la donnée. | Les outils en self-service permettent aux métiers, analystes et ingénieurs d’accéder et d’analyser les données sans dépendre constamment des équipes IT. |\n",
    "| **Gouvernance et sécurité des données** | Gouvernance minimale ou manuelle, entraînant des silos de données et une conformité incohérente. | Fonctionnalités de gouvernance intégrées (catalogues de données, traçabilité, contrôle d’accès par rôles – RBAC) assurant conformité et intégrité des données. |\n",
    "| **Analytics et IA / ML** | Principalement orienté vers le reporting et les tableaux de bord basiques, avec peu de capacités prédictives. | Permet des analyses avancées, des dashboards temps réel et une intégration fluide des modèles IA/ML pour des analyses prédictives et prescriptives. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b077156",
   "metadata": {},
   "source": [
    "#### Les 2 écoles de pensée : Inmon versus Kimball"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4256fa77",
   "metadata": {},
   "source": [
    "L'histoire du Data Warehouse est marquée par deux visions différentes, portées par deux pionniers : Bill Inmon et Ralph Kimball. Comprendre cette opposition est fondamental car elle structure encore aujourd'hui notre façon de penser l'architecture des données."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431c6671",
   "metadata": {},
   "source": [
    "##### **L'approche Inmon : Top-Down**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcf3f5c",
   "metadata": {},
   "source": [
    "Bill Inmon, souvent appelé \"le père du Data Warehouse\", prône une approche centralisée et méthodique. Sa philosophie peut se résumer ainsi : avant de pouvoir analyser quoi que ce soit, il faut d'abord construire un Data Warehouse d'entreprise complet et parfaitement normalisé. Ce n'est qu'ensuite qu'on en dérive des Data Marts spécialisés par département.\n",
    "\n",
    "<p align=\"center\" width=\"100%\">\n",
    "    <img width=\"50%\" height=\"50%\" src=\"../assets/dwh_v_inmon.png\" alt=\"Inmon Data Warehouse Architecture\"> \n",
    "</p>\n",
    "\n",
    "La normalisation est au cœur de cette approche. Normaliser signifie organiser les données de manière à éliminer toute redondance. Prenons un exemple concret avec un site e-commerce. Sans normalisation, chaque fois qu'un client passe commande, on répéterait toutes ses informations : son nom, son email, son adresse. Si Marie Dubois passe 10 commandes, on aurait 10 fois les mêmes informations. Avec la normalisation à la Inmon, on crée une table \"Clients\" où Marie apparaît une seule fois avec toutes ses informations, et une table \"Commandes\" où chaque commande pointe simplement vers Marie par un identifiant.\n",
    "\n",
    "L'intégrité des données est maximale : si l'email de Marie change, on modifie une seule ligne dans la table Clients, et automatiquement toutes ses commandes voient le changement. On a une seule source de vérité pour toute l'entreprise : le département ventes et le département marketing utilisent exactement la même définition de \"client\". L'architecture est robuste et évolutive : elle peut tenir vingt ans sans nécessiter de refonte majeure.\n",
    "\n",
    "Mais ces avantages ont un prix. Le temps de mise en œuvre est considérable : il faut compter entre 12 et 24 mois avant de voir les premiers résultats. Pourquoi si long ? Parce qu'il faut d'abord analyser toute l'entreprise, comprendre tous les processus métier, harmoniser toutes les définitions, avant même de commencer à construire quoi que ce soit. Le coût est également élevé, nécessitant une équipe importante d'architectes, de DBA (Database Administrators), et d'analystes métier. Enfin, les utilisateurs métier doivent attendre longtemps avant de pouvoir utiliser le système, ce qui peut générer de la frustration et même conduire à l'abandon du projet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9a86c7",
   "metadata": {},
   "source": [
    "##### **L'approche Kimball : Bottom-Up**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d3e0d4",
   "metadata": {},
   "source": [
    "Ralph Kimball, le pragmatique, propose une vision radicalement différente. Sa philosophie : pourquoi attendre 2 ans pour avoir un système complet alors qu'on pourrait commencer à créer de la valeur dès maintenant ? Construisons d'abord des Data Marts dimensionnels par domaine métier, et intégrons-les progressivement.\n",
    "\n",
    "Avec Kimball, on commence par identifier le processus métier le plus important, par exemple les ventes, et on construit rapidement un Data Mart spécifiquement pour ce domaine. Ce Data Mart utilise ce qu'on appelle un \"modèle en étoile\", que nous détaillerons dans la section sur la modélisation. L'idée est d'accepter une certaine redondance des données pour optimiser la performance des analyses. Une fois le Data Mart Ventes en production, on passe au Data Mart Marketing, puis au Data Mart RH, et ainsi de suite.\n",
    "\n",
    "\n",
    "<p align=\"center\" width=\"100%\">\n",
    "    <img width=\"50%\" height=\"50%\" src=\"../assets/dwh_v_kimball.png\" alt=\"Kimball Data Mart Architecture\"> \n",
    "</p>\n",
    "\n",
    "\n",
    "Le concept clé chez Kimball est le \"Bus Dimensionnel\". C'est une métaphore empruntée aux transports en commun : différentes lignes de bus (les Data Marts) partagent les mêmes arrêts (les dimensions). Concrètement, si le Data Mart Ventes et le Data Mart Marketing utilisent tous deux une dimension \"Client\" avec exactement la même structure et les mêmes définitions, ils pourront communiquer et permettre des analyses croisées. C'est ce qu'on appelle des dimensions \"conformées\".\n",
    "\n",
    "Les avantages de l'approche Kimball sont particulièrement attractifs pour les organisations qui veulent des résultats rapides. Le retour sur investissement est rapide : un premier Data Mart peut être en production en quatre à huit semaines. Les requêtes sont simples et performantes : au lieu de devoir croiser six ou sept tables comme dans l'approche Inmon, on en croise généralement deux ou trois. Le modèle est intuitif pour les utilisateurs métier : la structure en étoile est facile à comprendre, même pour des non-techniciens. Enfin, l'approche est itérative et agile : on peut commencer petit, prouver la valeur, puis étendre progressivement.\n",
    "\n",
    "Mais Kimball a aussi ses inconvénients. La redondance assumée coûte en espace de stockage : les informations sur Marie Dubois peuvent être répétées dans plusieurs Data Marts. Il y a un risque de créer des silos si les Data Marts ne respectent pas le principe des dimensions conformées : le département ventes et le département marketing pourraient finir par avoir des définitions légèrement différentes de \"client\", rendant les analyses croisées impossibles. Enfin, la maintenance devient complexe à grande échelle : gérer vingt Data Marts différents demande une coordination importante."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5af9f2",
   "metadata": {},
   "source": [
    "##### **Comparaison et choix**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f34699",
   "metadata": {},
   "source": [
    "Aujourd'hui, le débat **Inmon** versus **Kimball** n'est plus aussi tranché qu'avant. La plupart des organisations adoptent une approche hybride, empruntant le meilleur des deux mondes. Les grandes entreprises avec des exigences réglementaires strictes (_banques_, _assurances_) tendent encore vers Inmon pour son intégrité et sa robustesse. Les startups, PME et organisations agiles préfèrent généralement Kimball pour sa rapidité et sa flexibilité. Et comme nous le verrons plus loin, le **_Modern Data Stack_** avec son architecture en couches (**Bronze**, **Silver**, **Gold**) permet de combiner élégamment les deux approches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecf5c38",
   "metadata": {},
   "source": [
    "### **Le Data Lake : L'entrepôt flexible**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22ef04e",
   "metadata": {},
   "source": [
    "Le Lakehouse est une innovation récente, popularisée par Databricks à partir de 2020, qui tente de résoudre le dilemme entre Data Warehouse et Data Lake. L'idée est simple mais puissante : et si on pouvait avoir la flexibilité et le coût du Data Lake tout en conservant les performances et la fiabilité du Data Warehouse ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb05f124",
   "metadata": {},
   "source": [
    "<p align=\"center\" width=\"100%\">\n",
    "    <img width=\"70%\" height=\"70%\" src=\"../assets/lakehouse.png\" alt=\"Kimball Data Mart Architecture\"> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b675c329",
   "metadata": {},
   "source": [
    "Le **Lakehouse** repose sur des technologies comme Delta Lake (**Databricks**), Apache Iceberg, ou Apache Hudi, qui ajoutent une couche de gestion transactionnelle au-dessus du stockage objet traditionnel. Concrètement, cela signifie qu'on peut stocker des fichiers dans leur format natif (comme dans un Data Lake), mais avec des garanties de qualité et de cohérence (comme dans un Data Warehouse).\n",
    "\n",
    "Les avantages sont nombreux. On conserve le stockage peu coûteux du Data Lake. On obtient des performances élevées pour les requêtes, comparables à un Data Warehouse. On bénéficie de transactions ACID (Atomicité, Cohérence, Isolation, Durabilité), qui garantissent que les données restent cohérentes même en cas de problème. On peut même faire du \"Time Travel\", c'est-à-dire revenir à l'état des données à un moment précis du passé. Et surtout, on peut gérer tous types de données : structurées, semi-structurées, non structurées.\n",
    "\n",
    "Le **Lakehouse** est devenu le standard du __Modern Data Stack__. C'est l'architecture de choix pour les entreprises qui veulent à la fois faire des analyses métier classiques et du Machine Learning sur des données diverses. C'est aussi l'architecture qui permet de mettre en œuvre l'approche en couches **Bronze-Silver-Gold** que nous allons maintenant découvrir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a267b62",
   "metadata": {},
   "source": [
    "### **L'Architecture Médaillon : Bronze, Silver, Gold**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfb390f",
   "metadata": {},
   "source": [
    "L'architecture Médaillon, aussi appelée architecture en couches ou multi-hop, est devenue un standard dans le Modern Data Stack, particulièrement avec Databricks. L'idée est d'organiser les données en trois niveaux de raffinement progressif, comme si on purifiait un métal brut pour en faire de l'or."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228f2752",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5e9fb6c",
   "metadata": {},
   "source": [
    "#### **La couche Bronze : Les données brutes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b1be91",
   "metadata": {},
   "source": [
    "La couche Bronze est le point d'entrée des données dans le système. Ici, on applique la philosophie du Data Lake dans sa forme la plus pure : on stocke tout, tel quel, sans transformation. Si vous recevez un fichier JSON de votre API Shopify, vous le stockez exactement comme il est arrivé. Si vous collectez des logs de votre site web, vous les stockez dans leur format brut. Si vous extrayez des données de votre ERP, vous les copiez telles quelles.\n",
    "\n",
    "L'objectif de la couche Bronze n'est pas de rendre les données utilisables, mais de les sécuriser et de les archiver. C'est votre assurance : si quelque chose se passe mal dans les transformations suivantes, vous pouvez toujours revenir à la source originale. C'est aussi votre flexibilité : si vous découvrez plus tard que vous avez besoin d'une information que vous n'aviez pas utilisée initialement, elle est encore là, dans son format d'origine.\n",
    "\n",
    "Pour reprendre notre exemple e-commerce, la couche Bronze contiendrait les fichiers JSON bruts extraits de l'API Shopify toutes les heures, les fichiers de logs du site web collectés en temps réel, et les exports CSV du service client effectués quotidiennement. Aucune transformation, aucune validation, juste le stockage brut."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71bd250",
   "metadata": {},
   "source": [
    "#### **La couche Silver : Les données nettoyées**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091ac75f",
   "metadata": {},
   "source": [
    "La couche Silver est celle où commence le vrai travail de transformation. Ici, on nettoie, on valide, on structure et on enrichit les données. L'objectif est de passer de données brutes potentiellement incomplètes ou erronées à des données de qualité, prêtes à être utilisées.\n",
    "\n",
    "Les transformations typiques incluent la suppression des doublons (si Marie Dubois apparaît trois fois dans les données sources, on ne garde qu'une seule ligne), la validation des formats (vérifier que les emails ont bien un arobase, que les dates sont cohérentes), la correction des erreurs connues, la standardisation (mettre tous les pays dans le même format, normaliser les adresses), et les jointures entre différentes sources (relier les commandes avec les informations clients).\n",
    "\n",
    "Dans notre exemple e-commerce, la couche Silver contiendrait une table \"customers\" nettoyée avec un seul enregistrement par client, une table \"orders\" avec des dates validées et standardisées, et une table \"products\" avec des catégories harmonisées. Si dans les données brutes un produit était catégorisé tantôt \"électronique\", tantôt \"electronique\" sans accent, tantôt \"Électronique\" avec une majuscule, dans Silver on aurait une seule catégorie propre : \"Électronique\".\n",
    "\n",
    "C'est dans la couche Silver qu'on applique généralement les principes de normalisation de l'approche Inmon. On crée des tables structurées avec des relations claires, on élimine les redondances, on garantit l'intégrité référentielle. Silver devient votre source de vérité pour toute l'entreprise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf6d8a3",
   "metadata": {},
   "source": [
    "#### **La couche Gold : Les données métier**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e696a168",
   "metadata": {},
   "source": [
    "La couche Gold est le niveau le plus haut de raffinement. Ici, les données ne sont plus organisées pour refléter la structure technique ou les sources d'origine, mais pour répondre directement aux besoins métier. On crée des agrégations, on calcule des métriques, on construit des modèles dimensionnels optimisés pour l'analyse.\n",
    "\n",
    "C'est dans Gold qu'on applique les principes de l'approche Kimball. On construit des modèles en étoile avec des tables de faits et des dimensions, spécifiquement pensés pour alimenter des dashboards Power BI, des rapports Tableau, ou des analyses ad-hoc. Les données sont pré-agrégées lorsque c'est pertinent : plutôt que de recalculer le chiffre d'affaires mensuel à chaque fois, on le calcule une fois et on le stocke.\n",
    "\n",
    "Dans notre exemple e-commerce, la couche Gold contiendrait un modèle en étoile pour les ventes (que nous détaillerons dans la section suivante), avec des métriques précalculées comme le chiffre d'affaires par région et par mois, le panier moyen par segment client, ou le taux de conversion par campagne marketing. Ces données sont prêtes à être consommées directement par Power BI pour créer des tableaux de bord.\n",
    "\n",
    "L'architecture Médaillon incarne parfaitement l'approche hybride moderne : Bronze apporte la flexibilité du Data Lake, Silver la rigueur du Data Warehouse à la Inmon, et Gold l'orientation métier du Data Warehouse à la Kimball."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
