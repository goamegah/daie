{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b787a81f",
   "metadata": {},
   "source": [
    "# **Introduction au Modern Data Stack**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c531f9",
   "metadata": {},
   "source": [
    "Nous vivons une époque où les données sont devenues l'or noir du 21ème siècle. Chaque jour, des milliards d'interactions génèrent des volumes de données astronomiques : achats en ligne, posts sur les réseaux sociaux, capteurs connectés, transactions bancaires, visionnages de vidéos. Mais collecter des données ne suffit pas. Le véritable défi est de les transformer en informations exploitables pour prendre de meilleures décisions. C'est précisément là qu'intervient le Modern Data Stack.\n",
    "\n",
    "Le terme \"Modern Data Stack\" désigne l'ensemble des technologies, architectures et pratiques actuelles pour collecter, stocker, transformer et analyser les données. Le mot \"moderne\" n'est pas anodin : il marque une rupture avec les approches traditionnelles, héritées des années 1990-2000, qui ne sont plus adaptées aux enjeux actuels. Ce chapitre vous propose de comprendre pourquoi nous avons besoin d'un Modern Data Stack, comment nous en sommes arrivés là, et quels sont les acteurs majeurs de cet écosystème."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc0388d",
   "metadata": {},
   "source": [
    "## **1. Pourquoi le Modern Data Stack ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768d2dcc",
   "metadata": {},
   "source": [
    "### **1.1 Les limites des approches traditionnelles**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1de72de",
   "metadata": {},
   "source": [
    "Pour comprendre pourquoi le Modern Data Stack s'est imposé, il faut d'abord comprendre les limites des systèmes traditionnels. Dans les années 1990-2000, la plupart des entreprises utilisaient des systèmes dits \"on-premise\", c'est-à-dire hébergés dans leurs propres locaux, avec des serveurs physiques installés dans des salles informatiques climatisées.\n",
    "\n",
    "Imaginez une grande banque au début des années 2000. Elle possède plusieurs dizaines de serveurs pour héberger son Data Warehouse, construit avec des technologies propriétaires comme Oracle ou Teradata. L'investissement initial est colossal : achat des serveurs, installation, climatisation, électricité, équipes techniques dédiées. Et si la banque a besoin de plus de puissance de calcul pour traiter davantage de données ? Il faut commander de nouveaux serveurs, attendre leur livraison (plusieurs semaines), les installer, les configurer. Ce processus peut prendre des mois.\n",
    "\n",
    "De plus, ces systèmes traditionnels étaient conçus pour un type de données bien précis : les données structurées, c'est-à-dire des tableaux avec des lignes et des colonnes. Or, dès les années 2010, l'explosion du web, des réseaux sociaux et des objets connectés a généré une diversité de données que ces systèmes ne pouvaient pas gérer : vidéos, images, logs JSON, fichiers texte non structurés, flux temps réel.\n",
    "\n",
    "Enfin, le coût était prohibitif. Une licence Teradata ou Oracle pouvait coûter des centaines de milliers, voire des millions d'euros par an. Seules les très grandes entreprises pouvaient se permettre de tels investissements. Les PME et startups n'avaient tout simplement pas accès à ces technologies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc79605b",
   "metadata": {},
   "source": [
    "### **1.2 Les promesses du Modern Data Stack**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c14e40",
   "metadata": {},
   "source": [
    "Le Modern Data Stack répond à ces limitations en s'appuyant sur quatre piliers fondamentaux.\n",
    "\n",
    "**Le Cloud computing** constitue le premier pilier. Au lieu de posséder et maintenir ses propres serveurs, une entreprise peut louer de la puissance de calcul et du stockage auprès de fournisseurs comme AWS, Azure ou Google Cloud. L'avantage est immédiat : plus besoin d'investissement initial massif, on ne paie que ce qu'on utilise. Et surtout, l'élasticité est totale : besoin de traiter 10 fois plus de données ce mois-ci ? On augmente la puissance en quelques clics, puis on la réduit le mois suivant. Cette flexibilité était impensable avec les systèmes on-premise.\n",
    "\n",
    "**La séparation stockage-calcul** est le deuxième pilier. Traditionnellement, stockage et puissance de calcul étaient liés : pour avoir plus de capacité de traitement, il fallait acheter plus de serveurs avec plus de disques durs. Le Modern Data Stack découple ces deux aspects. On peut stocker des pétaoctets de données à très faible coût dans des solutions comme AWS S3, et ne mobiliser de la puissance de calcul que lorsqu'on en a besoin pour analyser ces données. C'est comme avoir une immense bibliothèque (stockage) et pouvoir embaucher des équipes de lecture (calcul) uniquement quand on veut consulter des livres.\n",
    "\n",
    "**L'ouverture et l'interopérabilité** constituent le troisième pilier. Les systèmes traditionnels étaient souvent des \"boîtes noires\" propriétaires où l'on était captif d'un fournisseur unique. Le Modern Data Stack privilégie les formats ouverts (Parquet, Delta, Iceberg), les API standards, et la possibilité de faire communiquer différents outils entre eux. Vous pouvez utiliser dbt pour transformer vos données, Airflow pour orchestrer, Looker pour visualiser, et tout cela fonctionne ensemble de manière fluide.\n",
    "\n",
    "**L'accessibilité démocratisée** est le quatrième pilier. Grâce au Cloud et aux modèles de tarification à l'usage, même une startup de trois personnes peut utiliser les mêmes technologies que Netflix ou Spotify. Il n'y a plus de barrière financière à l'entrée. Cette démocratisation a accéléré l'innovation et permis à des milliers d'entreprises de devenir data-driven.\n",
    "\n",
    "\n",
    "| Dimension | Legacy Data Stack | Modern Data Stack |\n",
    "|---------|------------------|------------------|\n",
    "| **Infrastructure** | Basée sur des serveurs on-premise, nécessitant des investissements matériels importants et une maintenance continue. La montée en charge est limitée par les ressources physiques disponibles. | Infrastructure cloud-native (AWS, GCP, Azure) offrant scalabilité, élasticité et réduction des coûts initiaux. Les équipes peuvent augmenter ou réduire les ressources à la demande. |\n",
    "| **Ingestion des données** | Reposait principalement sur du traitement batch avec des planifications fixes. Les sources étaient limitées aux systèmes internes, avec des intégrations complexes pour en ajouter de nouvelles. | Supporte à la fois le streaming temps réel (ex. Apache Kafka) et l’ingestion batch. Des outils modernes comme Airbyte ou Fivetran s’intègrent facilement aux APIs, plateformes SaaS et objets connectés. |\n",
    "| **Stockage** | Utilisation de bases de données relationnelles rigides et coûteuses, avec une scalabilité et une flexibilité limitées. | Utilise des data warehouses cloud (ex. Snowflake, BigQuery) et des data lakes permettant un stockage élastique, le support des données structurées et non structurées, et de hautes performances de requêtage. |\n",
    "| **Traitement** | Les pipelines ETL impliquaient souvent du scripting manuel et manquaient de modularité, entraînant des temps de traitement lents et un taux d’erreurs plus élevé. | Les pipelines ELT s’appuient sur des outils comme dbt pour des transformations modulaires et déclaratives. Le traitement est automatisé, plus rapide et plus facile à déboguer. |\n",
    "| **Flexibilité et intégration** | Les systèmes propriétaires entraînaient un fort verrouillage fournisseur, rendant les intégrations complexes et coûteuses. | Les outils open-source et orientés API garantissent interopérabilité et flexibilité, permettant de composer des stacks personnalisées sans dépendre d’un seul fournisseur. |\n",
    "| **Modèle de coûts** | Fort investissement initial (CapEx) pour le matériel et les licences logicielles. | Modèle de paiement à l’usage (pay-as-you-go) réduisant les barrières financières et alignant les coûts sur la consommation réelle. |\n",
    "| **Accessibilité** | Accès centralisé géré par les équipes IT, créant des goulets d’étranglement et limitant la démocratisation de la donnée. | Les outils en self-service permettent aux métiers, analystes et ingénieurs d’accéder et d’analyser les données sans dépendre constamment des équipes IT. |\n",
    "| **Gouvernance et sécurité des données** | Gouvernance minimale ou manuelle, entraînant des silos de données et une conformité incohérente. | Fonctionnalités de gouvernance intégrées (catalogues de données, traçabilité, contrôle d’accès par rôles – RBAC) assurant conformité et intégrité des données. |\n",
    "| **Analytics et IA / ML** | Principalement orienté vers le reporting et les tableaux de bord basiques, avec peu de capacités prédictives. | Permet des analyses avancées, des dashboards temps réel et une intégration fluide des modèles IA/ML pour des analyses prédictives et prescriptives. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2f5e75",
   "metadata": {},
   "source": [
    "### **1.3 Les enjeux métier**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324ffe25",
   "metadata": {},
   "source": [
    "Au-delà des aspects techniques, le Modern Data Stack répond à des enjeux métier concrets. Dans un monde où la concurrence est féroce, les entreprises qui prennent les meilleures décisions le plus rapidement gagnent. Imaginez deux retailers concurrents. Le premier utilise encore des rapports Excel préparés manuellement une fois par mois. Le second a mis en place un Modern Data Stack qui lui permet de voir en temps réel les ventes par magasin, d'identifier immédiatement les produits qui se vendent mal, et d'ajuster ses prix et promotions dynamiquement. Qui pensez-vous aura l'avantage compétitif ?\n",
    "\n",
    "Le Modern Data Stack permet aussi de répondre à la question : \"Qui sont vraiment mes clients ?\" En consolidant les données du site web, de l'application mobile, des magasins physiques, du service client, une entreprise peut avoir une vue à 360 degrés de chaque client. Cette connaissance approfondie permet de personnaliser l'expérience, d'anticiper les besoins, de réduire le churn (l'attrition client).\n",
    "\n",
    "Enfin, le Modern Data Stack est le socle indispensable pour l'Intelligence Artificielle et le Machine Learning. Entraîner un modèle d'IA nécessite des volumes massifs de données de qualité, accessibles rapidement. Les architectures modernes rendent cela possible, là où les systèmes traditionnels ne pouvaient tout simplement pas suivre."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e219535",
   "metadata": {},
   "source": [
    "## **2. Évolution historique - Des bases de données au Modern Data Stack**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521e3f61",
   "metadata": {},
   "source": [
    "### **2.1 Les années 1980-1990 : L'ère des bases de données relationnelles**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880a36ac",
   "metadata": {},
   "source": [
    "L'histoire commence dans les années 1970-1980 avec l'invention des bases de données relationnelles par Edgar Codd chez IBM. Le concept est révolutionnaire pour l'époque : organiser les données dans des tables avec des lignes et des colonnes, et utiliser un langage déclaratif, SQL, pour interroger ces données. Des produits comme Oracle (1979), DB2 d'IBM, et plus tard SQL Server de Microsoft dominent le marché.\n",
    "\n",
    "Ces bases de données répondent parfaitement aux besoins des systèmes transactionnels : gérer les comptes bancaires, enregistrer les commandes clients, suivre les stocks. On parle de systèmes OLTP (Online Transaction Processing), optimisés pour traiter des milliers de petites transactions par seconde. Mais ces systèmes ne sont pas conçus pour l'analyse. Essayer de calculer le chiffre d'affaires des cinq dernières années en croisant plusieurs tables transactionnelles peut prendre des heures et ralentir tout le système."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f531e35",
   "metadata": {},
   "source": [
    "### **2.2 Les années 1990-2000 : L'avènement du Data Warehouse**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4699c648",
   "metadata": {},
   "source": [
    "C'est pour résoudre ce problème que Bill Inmon conceptualise le Data Warehouse au début des années 1990. L'idée est de créer un système séparé, dédié uniquement à l'analyse, alimenté par les bases de données transactionnelles. La nuit, des processus ETL (Extract-Transform-Load) copient les données des systèmes sources vers le Data Warehouse, les nettoient, les transforment, et les organisent pour faciliter l'analyse.\n",
    "\n",
    "Des éditeurs comme Teradata (spécialisé dès le départ dans les Data Warehouses) et Oracle avec ses solutions décisionnelles investissent massivement ce marché. Les grandes entreprises construisent des Data Warehouses monumentaux, parfois sur plusieurs années, avec des équipes de dizaines de personnes.\n",
    "\n",
    "Ralph Kimball, dans la même période, propose une approche alternative plus pragmatique avec ses Data Marts dimensionnels et ses modèles en étoile. Cette dualité Inmon-Kimball, que nous avons explorée en détail au Chapitre 2, structure toute l'industrie du décisionnel pendant deux décennies.\n",
    "\n",
    "Mais ces systèmes ont tous un point commun : ils sont on-premise, propriétaires, coûteux, et conçus uniquement pour des données structurées. Ils ne peuvent pas anticiper la révolution qui s'annonce.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed541ef",
   "metadata": {},
   "source": [
    "### **2.3 Les années 2010 : La révolution Big Data et le Data Lake**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bbd1af",
   "metadata": {},
   "source": [
    "Le début des années 2010 marque une rupture. L'explosion du web 2.0, des réseaux sociaux (Facebook, Twitter), des smartphones, et des objets connectés génère des volumes de données sans précédent. On ne parle plus de gigaoctets ou de téraoctets, mais de pétaoctets. Et surtout, ces données sont diverses : logs serveurs au format JSON, vidéos, images, textes non structurés.\n",
    "\n",
    "Les Data Warehouses traditionnels ne peuvent pas gérer cette diversité et cette échelle. C'est dans ce contexte que naît le concept de Data Lake, popularisé par des entreprises comme Yahoo, LinkedIn et Netflix. L'idée fondamentale est simple : au lieu d'essayer de structurer les données avant de les stocker, stockons-les telles quelles dans un grand réservoir, et nous déciderons de la structure au moment de les analyser.\n",
    "\n",
    "Hadoop, un framework open-source inspiré des technologies de Google, devient le symbole de cette révolution Big Data. Il permet de stocker et de traiter des volumes massifs de données sur des clusters de serveurs standard (et donc bon marché), plutôt que sur des serveurs propriétaires hors de prix.\n",
    "\n",
    "![image.png](../assets/arch_evol.png)\n",
    "\n",
    "Mais le Data Lake apporte aussi ses propres problèmes. Sans gouvernance appropriée, il se transforme rapidement en \"Data Swamp\", un marécage où personne ne sait plus ce qui est stocké ni dans quel état. Les performances pour les requêtes analytiques sont souvent décevantes. Et la complexité technique (installer et maintenir un cluster Hadoop) reste élevée."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd09597",
   "metadata": {},
   "source": [
    "### **2.4 Les années 2015-2020 : Le Cloud change la donne**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35c726f",
   "metadata": {},
   "source": [
    "L'arrivée à maturité des plateformes Cloud (AWS, Azure, Google Cloud) change radicalement le paysage. Plusieurs innovations majeures apparaissent simultanément.\n",
    "\n",
    "Snowflake, fondé en 2012 et lancé en 2015, réinvente le Data Warehouse pour le Cloud. Entièrement managé (plus besoin d'installer ou de maintenir quoi que ce soit), avec séparation stockage-calcul, élasticité instantanée, Snowflake démontre qu'on peut avoir les performances d'un Data Warehouse traditionnel sans ses inconvénients. Le succès est fulgurant : en quelques années, Snowflake s'impose comme un acteur majeur.\n",
    "\n",
    "Google lance BigQuery en 2011, un Data Warehouse serverless où l'on ne se préoccupe même plus de la notion de serveurs ou de clusters. On écrit des requêtes SQL, BigQuery les exécute en mobilisant automatiquement les ressources nécessaires, et on ne paie que pour les données traitées.\n",
    "\n",
    "Databricks, fondé par les créateurs d'Apache Spark en 2013, propose une vision différente : unifier Data Lake et Data Warehouse dans ce qu'ils nomment le \"Lakehouse\". Avec Delta Lake, Databricks ajoute des capacités transactionnelles (ACID) au-dessus du stockage objet, permettant d'avoir à la fois la flexibilité du Data Lake et les garanties de qualité du Data Warehouse.\n",
    "\n",
    "En parallèle, l'écosystème d'outils se structure. dbt (data build tool), lancé en 2016, révolutionne la transformation de données avec une approche code-first et des principes de génie logiciel (versioning, tests, documentation). Fivetran et Airbyte simplifient radicalement l'ingestion de données. Looker, Tableau et Power BI démocratisent la visualisation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1614081",
   "metadata": {},
   "source": [
    "### **2.5 2020 et au-delà : Le Modern Data Stack mature**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e531914f",
   "metadata": {},
   "source": [
    "Aujourd'hui, en 2025, le Modern Data Stack est mature et s'est imposé comme le standard. Il se caractérise par une architecture en couches (Bronze-Silver-Gold que nous avons vue au Chapitre 2), l'utilisation de technologies Cloud-native, l'approche ELT plutôt qu'ETL, et l'intégration native de capacités de Machine Learning et d'IA.\n",
    "\n",
    "Les frontières entre les catégories historiques (Data Warehouse, Data Lake) s'estompent. Snowflake ajoute des capacités Data Lake, Databricks améliore ses performances SQL pour concurrencer les Data Warehouses purs. Google BigQuery continue d'innover avec BigQuery ML pour le Machine Learning directement dans le Data Warehouse.\n",
    "\n",
    "L'essentiel n'est plus tant la technologie choisie que l'architecture globale et les pratiques : capacité à gérer tous types de données, gouvernance intégrée, performance et élasticité, coûts maîtrisés, et surtout, capacité à créer de la valeur métier rapidement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144b510c",
   "metadata": {},
   "source": [
    "## **3. Les acteurs majeurs de l'écosystème**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4c47cc",
   "metadata": {},
   "source": [
    "### **3.1 Les Cloud Providers : Les fondations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25b0b91",
   "metadata": {},
   "source": [
    "Trois géants dominent le marché du Cloud computing et servent de socle au Modern Data Stack.\n",
    "\n",
    "**Amazon Web Services (AWS)**, pionnier du Cloud, lancé en 2006, reste le leader mondial avec environ 34% de parts de marché. AWS propose une palette impressionnante de services : S3 pour le stockage objet (devenu un standard de facto), Redshift pour le Data Warehouse, Glue pour l'ETL, Athena pour requêter des données directement dans S3. L'avantage d'AWS est sa maturité et l'étendue de son catalogue, avec plus de 200 services. L'inconvénient est parfois une certaine complexité et une facturation qui peut être difficile à prévoir.\n",
    "\n",
    "**Microsoft Azure** talonne AWS avec environ 21% de parts de marché. L'atout majeur d'Azure est son intégration avec l'écosystème Microsoft, particulièrement pertinent pour les entreprises qui utilisent déjà Office 365, Teams, et Power BI. Azure propose Azure Data Lake Storage (ADLS) pour le stockage, Synapse Analytics comme Data Warehouse, Data Factory pour l'orchestration. Pour les entreprises ayant des contrats Microsoft existants, Azure offre souvent une continuité attractive.\n",
    "\n",
    "**Google Cloud Platform (GCP)** arrive troisième avec environ 10% de parts de marché, mais avec des atouts techniques indéniables. BigQuery, lancé en 2011, reste une référence en termes de performances et de simplicité d'utilisation. Google a également une forte expertise en Machine Learning (TensorFlow, Vertex AI) qui s'intègre naturellement avec les solutions data. GCP est souvent privilégié par les startups tech et les entreprises qui veulent les technologies les plus avancées en IA.\n",
    "\n",
    "![image.png](../assets/cloud_part.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2ff2ce",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
