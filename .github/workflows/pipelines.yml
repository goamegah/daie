name: CI/CD Pipeline

on:
  push:
    branches:
      - main
      - develop
  pull_request:
    branches:
      - main
      - develop
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environnement de d√©ploiement'
        required: true
        default: 'dev'
        type: choice
        options:
          - dev
          - test
          - prod

env:
  PYTHON_VERSION: '3.11'

permissions:
  contents: read
  checks: write
  pull-requests: write

jobs:
  # ============================================
  # JOB 1: Lint & Qualit√© du code
  # ============================================
  
  lint:
    name: Lint & Qualit√© du code
    runs-on: ubuntu-latest
    steps:
      - name: Checkout du code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Installation des d√©pendances (lint)
        run: |
          python -m pip install --upgrade pip
          pip install pylint

      - name: V√©rification avec Pylint
        run: pylint daie/ --exit-zero --output-format=colorized
        continue-on-error: true
  
  # ============================================
  # JOB 2: Tests unitaires
  # ============================================
  test:
    name: Tests unitaires
    runs-on: ubuntu-latest
    needs: lint
    steps:
      - name: Checkout du code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Setup Java (requis pour PySpark)
        uses: actions/setup-java@v4
        with:
          distribution: 'temurin'
          java-version: '11'

      - name: Installation des d√©pendances de test
        run: |
          python -m pip install --upgrade pip
          pip install -r test-requirements.txt

      - name: Installation du package en mode d√©veloppement (sans d√©pendances)
        run: pip install -e . --no-deps

      - name: V√©rification de l'installation PySpark
        run: |
          python -c "from pyspark.sql import SparkSession; print('PySpark OK')"
          python -c "import importlib.util; print('pyspark.dbutils exists:', importlib.util.find_spec('pyspark.dbutils') is not None)"

      - name: Tests unitaires avec pytest
        run: |
          pytest tests/ \
            -v \
            --junitxml=test-results.xml \
            --cov=daie \
            --cov-report=xml \
            --cov-report=html \
            --cov-report=term-missing

      - name: Publication des r√©sultats de tests
        uses: dorny/test-reporter@v1
        if: always() && github.event_name != 'pull_request'
        with:
          name: Test Results
          path: test-results.xml
          reporter: java-junit
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Publication de la couverture
        uses: codecov/codecov-action@v4
        if: always()
        with:
          files: ./coverage.xml
          fail_ci_if_error: false

      - name: Upload coverage reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: coverage-report
          path: |
            coverage.xml
            htmlcov/

  # ============================================
  # JOB 3: Build du package
  # ============================================
  build:
    name: Build du package
    runs-on: ubuntu-latest
    needs: test
    steps:
      - name: Checkout du code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Installation des d√©pendances (build)
        run: |
          python -m pip install --upgrade pip
          pip install build twine wheel

      - name: Build du package wheel
        run: python -m build

      - name: V√©rification du package construit
        run: |
          ls -lh dist/
          twine check dist/*
        continue-on-error: true

      - name: Upload des artefacts de build
        uses: actions/upload-artifact@v4
        with:
          name: python-package
          path: dist/
          retention-days: 30

  # ============================================
  # JOB 4: D√©ploiement sur Databricks (Unity Catalog)
  # ============================================
  # deploy:
  #   name: D√©ploiement Databricks
  #   runs-on: ubuntu-latest
  #   needs: build
  #   if: github.event_name == 'workflow_dispatch'
  #   environment: ${{ github.event.inputs.environment }}
  #   env:
  #     # Variables dynamiques selon l'environnement
  #     DATABRICKS_HOST: ${{ vars.DATABRICKS_HOST }}
  #     DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
  #     UNITY_CATALOG: ${{ vars.UNITY_CATALOG }}
  #     UNITY_SCHEMA: ${{ vars.UNITY_SCHEMA }}
  #   steps:
  #     - name: Checkout du code
  #       uses: actions/checkout@v4

  #     - name: Setup Python
  #       uses: actions/setup-python@v5
  #       with:
  #         python-version: ${{ env.PYTHON_VERSION }}

  #     - name: Download des artefacts de build
  #       uses: actions/download-artifact@v4
  #       with:
  #         name: python-package
  #         path: dist/

  #     - name: Installation de Databricks CLI
  #       run: |
  #         pip install databricks-cli databricks-sdk

  #     - name: Configuration de Databricks CLI
  #       run: |
  #         echo "[DEFAULT]" > ~/.databrickscfg
  #         echo "host = ${{ env.DATABRICKS_HOST }}" >> ~/.databrickscfg
  #         echo "token = ${{ env.DATABRICKS_TOKEN }}" >> ~/.databrickscfg

  #     - name: Affichage de l'environnement de d√©ploiement
  #       run: |
  #         echo "üöÄ D√©ploiement vers l'environnement: ${{ github.event.inputs.environment }}"
  #         echo "üì¶ Databricks Host: ${{ env.DATABRICKS_HOST }}"
  #         echo "üìÅ Unity Catalog: ${{ env.UNITY_CATALOG }}"
  #         echo "üìÇ Unity Schema: ${{ env.UNITY_SCHEMA }}"
  #         ls -lh dist/

  #     - name: Upload du wheel vers Unity Catalog Volume
  #       run: |
  #         WHEEL_FILE=$(ls dist/*.whl | head -1)
  #         WHEEL_NAME=$(basename $WHEEL_FILE)
  #         VOLUME_PATH="/Volumes/${{ env.UNITY_CATALOG }}/${{ env.UNITY_SCHEMA }}/packages"
          
  #         echo "üì§ Upload de $WHEEL_NAME vers $VOLUME_PATH"
  #         databricks fs cp "$WHEEL_FILE" "dbfs:$VOLUME_PATH/$WHEEL_NAME" --overwrite
          
  #         echo "‚úÖ Package upload√© avec succ√®s"

  #     - name: V√©rification du d√©ploiement
  #       run: |
  #         VOLUME_PATH="/Volumes/${{ env.UNITY_CATALOG }}/${{ env.UNITY_SCHEMA }}/packages"
  #         echo "üìã Contenu du volume:"
  #         databricks fs ls "dbfs:$VOLUME_PATH"

  #     - name: Mise √† jour des jobs Databricks (optionnel)
  #       if: ${{ github.event.inputs.environment == 'prod' }}
  #       run: |
  #         echo "üîÑ Mise √† jour des jobs Databricks pour l'environnement de production..."
  #         # D√©commentez et adaptez selon vos besoins:
  #         # databricks jobs list --output JSON
  #         # databricks jobs update --job-id <JOB_ID> --json-file deployment/jobs/job_config.json
  #       continue-on-error: true