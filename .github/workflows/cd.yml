name: CD - DÃ©ploiement Databricks

on:
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environnement'
        required: true
        default: 'dev'
        type: choice
        options: [dev, test, prod]
      developer_name:
        description: 'Nom dÃ©veloppeur'
        required: false
        default: 'dev'
        type: string
      skip_tests:
        description: 'Ignorer tests'
        required: false
        default: false
        type: boolean
      deploy_metadata:
        description: 'ğŸ“¦ DÃ©ployer metadata'
        required: false
        default: false
        type: boolean
      deploy_config:
        description: 'âš™ï¸ DÃ©ployer config'
        required: false
        default: false
        type: boolean
      deploy_schema:
        description: 'ğŸ“‹ DÃ©ployer schema'
        required: false
        default: false
        type: boolean
      deploy_init_scripts:
        description: 'ğŸš€ DÃ©ployer init scripts'
        required: false
        default: true
        type: boolean
      manage_cluster:
        description: 'ğŸ–¥ï¸ GÃ©rer le cluster'
        required: false
        default: 'none'
        type: choice
        options: [none, create, delete]
      transfer_jobs_ownership:
        description: 'ğŸ” TransfÃ©rer ownership des jobs au SP'
        required: false
        default: false
        type: boolean

env:
  PYTHON_VERSION: '3.11'
  CATALOG_PREFIX: 'daie_chn'

permissions:
  contents: read

jobs:
  test:
    name: Tests
    runs-on: ubuntu-latest
    if: ${{ !(github.event.inputs.environment == 'dev' && github.event.inputs.skip_tests == 'true') }}
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: pip
      - uses: actions/setup-java@v4
        with:
          distribution: temurin
          java-version: '11'
      - run: |
          pip install -U pip
          pip install -r test-requirements.txt
          pip install -e . --no-deps
      - run: pytest tests/ -v --cov=src --cov-report=term-missing

  build:
    name: Build
    runs-on: ubuntu-latest
    needs: test
    if: always() && (needs.test.result == 'success' || needs.test.result == 'skipped')
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: pip
      - run: |
          pip install -U pip build wheel
          python -m build
          ls -lh dist/
      - uses: actions/upload-artifact@v4
        with:
          name: python-wheel
          path: dist/*.whl
          retention-days: 7
          if-no-files-found: error

  deploy:
    name: Deploy
    runs-on: ubuntu-latest
    needs: build
    environment: ${{ github.event.inputs.environment }}
    env:
      ENV_NAME: ${{ github.event.inputs.environment }}
      DEVELOPER_NAME: ${{ github.event.inputs.developer_name }}
      DATABRICKS_HOST: ${{ vars.DATABRICKS_HOST }}
      AZURE_CLIENT_ID: ${{ secrets.ARM_CLIENT_ID }}
      AZURE_CLIENT_SECRET: ${{ secrets.ARM_CLIENT_SECRET }}
      AZURE_TENANT_ID: ${{ secrets.ARM_TENANT_ID }}
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      - uses: actions/download-artifact@v4
        with:
          name: python-wheel
          path: dist/
      - name: Install SDK
        run: |
          pip install -U pip databricks-sdk
          python -c "import databricks.sdk; print('SDK OK')"
      
      - name: Create deploy script
        run: |
          cat > deploy.py << 'EOF'
          import os, sys
          from pathlib import Path
          from databricks.sdk import WorkspaceClient
          from databricks.sdk.core import DatabricksError
          
          # Nettoyer les variables conflictuelles
          for key in ['DATABRICKS_CLIENT_ID', 'DATABRICKS_CLIENT_SECRET']:
              if key in os.environ:
                  del os.environ[key]
          
          host = os.getenv('DATABRICKS_HOST')
          client_id = os.getenv('AZURE_CLIENT_ID')
          client_secret = os.getenv('AZURE_CLIENT_SECRET')
          tenant_id = os.getenv('AZURE_TENANT_ID')
          catalog = os.getenv('BRONZE_CATALOG')
          volume_path = os.getenv('VOLUME_PATH')
          wheel_file = os.getenv('WHEEL_FILE')
          wheel_name = os.getenv('WHEEL_NAME')
          
          print(f"ğŸ”§ Config:\n   Host: {host}\n   Catalog: {catalog}\n   Volume: {volume_path}\n   Package: {wheel_name}\n")
          
          print("ğŸ” Connecting...")
          w = WorkspaceClient(host=host, azure_client_id=client_id, azure_client_secret=client_secret, azure_tenant_id=tenant_id)
          user = w.current_user.me()
          print(f"âœ… Connected as: {user.user_name}\n")
          
          print(f"ğŸ“ Creating directory: {volume_path}")
          try:
              w.files.create_directory(volume_path)
              print("âœ… Created")
          except DatabricksError as e:
              if "RESOURCE_ALREADY_EXISTS" in str(e) or "already exists" in str(e).lower():
                  print("âš ï¸  Already exists")
              else:
                  print(f"âš ï¸  {e}")
          print()
          
          print(f"ğŸ“¤ Uploading {wheel_name}...")
          destination = f"{volume_path}/{wheel_name}"
          with open(wheel_file, 'rb') as f:
              w.files.upload(destination, f, overwrite=True)
          print(f"âœ… Uploaded to: {destination}\n")
          
          print("ğŸ” Verifying...")
          files = list(w.files.list_directory_contents(volume_path))
          print(f"Files in {volume_path}:")
          for f in files:
              size = f.file_size if hasattr(f, 'file_size') else 'N/A'
              print(f"  - {f.name} ({size} bytes)")
          print()
          
          print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
          print("â•‘   âœ… DEPLOYMENT SUCCESSFUL            â•‘")
          print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")
          print(f"ğŸ“¦ Install in notebook:\n%pip install {destination}\n")
          EOF
          chmod +x deploy.py
      
      - name: Compute vars
        id: vars
        run: |
          BRONZE_CATALOG="${CATALOG_PREFIX}_${ENV_NAME}_bronze"
          VOLUME_PATH="/Volumes/${BRONZE_CATALOG}/artifacts/packages/${DEVELOPER_NAME}"
          WHEEL_FILE=$(ls dist/*.whl | head -1)
          WHEEL_NAME=$(basename "$WHEEL_FILE")
          
          echo "BRONZE_CATALOG=${BRONZE_CATALOG}" >> $GITHUB_OUTPUT
          echo "VOLUME_PATH=${VOLUME_PATH}" >> $GITHUB_OUTPUT
          echo "WHEEL_FILE=${WHEEL_FILE}" >> $GITHUB_OUTPUT
          echo "WHEEL_NAME=${WHEEL_NAME}" >> $GITHUB_OUTPUT
          echo "BRONZE_CATALOG=${BRONZE_CATALOG}" >> $GITHUB_ENV
          echo "VOLUME_PATH=${VOLUME_PATH}" >> $GITHUB_ENV
          echo "WHEEL_FILE=${WHEEL_FILE}" >> $GITHUB_ENV
          echo "WHEEL_NAME=${WHEEL_NAME}" >> $GITHUB_ENV
          
          echo "ğŸ“¦ Vars: Catalog=${BRONZE_CATALOG}, Volume=${VOLUME_PATH}, Package=${WHEEL_NAME}"
      
      - name: Deploy wheel
        run: python deploy.py
      
      - name: Deploy init scripts
        if: ${{ github.event.inputs.deploy_init_scripts == 'true' }}
        run: python deployment/devops/deploy_artifacts.py init_scripts ${{ env.ENV_NAME }} ${{ env.DEVELOPER_NAME }}
      
      - name: Manage cluster (create)
        if: ${{ github.event.inputs.manage_cluster == 'create' }}
        run: python deployment/devops/manage_cluster.py create ${{ env.ENV_NAME }} ${{ env.DEVELOPER_NAME }}
      
      - name: Install on existing clusters
        if: ${{ github.event.inputs.manage_cluster != 'create' }}
        run: python deployment/devops/install_package_on_clusters.py ${{ env.ENV_NAME }} ${{ env.DEVELOPER_NAME }}
        continue-on-error: true
      
      - name: Manage cluster (delete)
        if: ${{ github.event.inputs.manage_cluster == 'delete' }}
        run: python deployment/devops/manage_cluster.py delete ${{ env.ENV_NAME }} ${{ env.DEVELOPER_NAME }}
      
      - name: Transfer jobs ownership to SP
        if: ${{ github.event.inputs.transfer_jobs_ownership == 'true' }}
        run: python deployment/devops/transfer_jobs_ownership.py ${{ env.ENV_NAME }}
      
      - name: Summary
        if: always()
        run: |
          echo ""
          echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
          echo "â•‘   ğŸ“Š SUMMARY                          â•‘"
          echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo "Environment: ${{ env.ENV_NAME }}"
          echo "Developer: ${{ env.DEVELOPER_NAME }}"
          echo "Package: ${{ steps.vars.outputs.WHEEL_NAME }}"
          echo "Location: ${{ steps.vars.outputs.VOLUME_PATH }}"
          echo "Cluster action: ${{ github.event.inputs.manage_cluster }}"
          echo "Jobs ownership transferred: ${{ github.event.inputs.transfer_jobs_ownership }}"
          echo ""

  deploy-artifacts:
    name: Deploy Artifacts
    runs-on: ubuntu-latest
    needs: build
    if: ${{ github.event.inputs.deploy_metadata == 'true' || github.event.inputs.deploy_config == 'true' || github.event.inputs.deploy_schema == 'true' || github.event.inputs.deploy_init_scripts == 'true' }}
    environment: ${{ github.event.inputs.environment }}
    env:
      ENV_NAME: ${{ github.event.inputs.environment }}
      DEVELOPER_NAME: ${{ github.event.inputs.developer_name }}
      DATABRICKS_HOST: ${{ vars.DATABRICKS_HOST }}
      AZURE_CLIENT_ID: ${{ secrets.ARM_CLIENT_ID }}
      AZURE_CLIENT_SECRET: ${{ secrets.ARM_CLIENT_SECRET }}
      AZURE_TENANT_ID: ${{ secrets.ARM_TENANT_ID }}
    steps:
      - uses: actions/checkout@v4
      
      - uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install SDK
        run: |
          pip install -U pip databricks-sdk
          python -c "import databricks.sdk; print('SDK OK')"
      
      - name: Deploy metadata
        if: ${{ github.event.inputs.deploy_metadata == 'true' }}
        run: python deployment/devops/deploy_artifacts.py metadata ${{ env.ENV_NAME }} ${{ env.DEVELOPER_NAME }}
      
      - name: Deploy config
        if: ${{ github.event.inputs.deploy_config == 'true' }}
        run: python deployment/devops/deploy_artifacts.py config ${{ env.ENV_NAME }} ${{ env.DEVELOPER_NAME }}
      
      - name: Deploy schema
        if: ${{ github.event.inputs.deploy_schema == 'true' }}
        run: python deployment/devops/deploy_artifacts.py schema ${{ env.ENV_NAME }} ${{ env.DEVELOPER_NAME }}
      
      - name: Deploy init scripts (artifacts job)
        if: ${{ github.event.inputs.deploy_init_scripts == 'true' }}
        run: python deployment/devops/deploy_artifacts.py init_scripts ${{ env.ENV_NAME }} ${{ env.DEVELOPER_NAME }}
      
      - name: Summary
        if: always()
        run: |
          echo ""
          echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
          echo "â•‘   ğŸ“Š ARTIFACTS DEPLOYMENT SUMMARY     â•‘"
          echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo "Environment: ${{ env.ENV_NAME }}"
          echo "Developer: ${{ env.DEVELOPER_NAME }}"
          echo "Deployed artifacts:"
          ${{ github.event.inputs.deploy_metadata == 'true' }} && echo "  âœ… metadata" || echo "  â­ï¸  metadata"
          ${{ github.event.inputs.deploy_config == 'true' }} && echo "  âœ… config" || echo "  â­ï¸  config"
          ${{ github.event.inputs.deploy_schema == 'true' }} && echo "  âœ… schema" || echo "  â­ï¸  schema"
          ${{ github.event.inputs.deploy_init_scripts == 'true' }} && echo "  âœ… init_scripts" || echo "  â­ï¸  init_scripts"
          echo ""