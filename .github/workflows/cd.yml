name: CD - D√©ploiement Databricks

on:
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environnement de d√©ploiement'
        required: true
        default: 'dev'
        type: choice
        options:
          - dev
          - test
          - prod
      developer_name:
        description: 'Nom du d√©veloppeur (ex: godwin). Laisser vide pour "dev" par d√©faut.'
        required: false
        type: string
      skip_tests:
        description: 'Ignorer les tests (uniquement pour dev)'
        required: false
        default: false
        type: boolean

env:
  PYTHON_VERSION: '3.11'
  CATALOG_PREFIX: 'daie_chn'

permissions:
  contents: read

jobs:
  test:
    name: Tests avant d√©ploiement
    runs-on: ubuntu-latest
    if: ${{ !(github.event.inputs.environment == 'dev' && github.event.inputs.skip_tests == 'true') }}
    steps:
      - name: Checkout du code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Setup Java (requis pour PySpark)
        uses: actions/setup-java@v4
        with:
          distribution: 'temurin'
          java-version: '11'

      - name: Installation des d√©pendances de test
        run: |
          python -m pip install --upgrade pip
          pip install -r test-requirements.txt

      - name: Installation du package
        run: pip install -e . --no-deps

      - name: Tests unitaires
        run: pytest tests/ -v

  build:
    name: Build du package
    runs-on: ubuntu-latest
    needs: [test]
    if: always() && (needs.test.result == 'success' || needs.test.result == 'skipped')
    steps:
      - name: Checkout du code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Installation des d√©pendances (build)
        run: |
          python -m pip install --upgrade pip
          pip install build wheel

      - name: Build du package wheel
        run: python -m build

      - name: Upload des artefacts de build
        uses: actions/upload-artifact@v4
        with:
          name: python-package
          path: dist/
          retention-days: 7

  deploy:
    name: D√©ploiement Databricks
    runs-on: ubuntu-latest
    needs: build
    environment: ${{ github.event.inputs.environment }}
    env:
      # Configuration Databricks
      DATABRICKS_HOST: ${{ vars.DATABRICKS_HOST }}
      # Service Principal credentials (depuis GitHub Secrets)
      ARM_CLIENT_ID: ${{ secrets.ARM_CLIENT_ID }}
      ARM_CLIENT_SECRET: ${{ secrets.ARM_CLIENT_SECRET }}
      ARM_TENANT_ID: ${{ secrets.ARM_TENANT_ID }}
      # Variables de d√©ploiement
      DEVELOPER_NAME: ${{ github.event.inputs.developer_name || 'dev' }}
      ENV_NAME: ${{ github.event.inputs.environment }}
    steps:
      - name: Checkout du code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Download des artefacts de build
        uses: actions/download-artifact@v4
        with:
          name: python-package
          path: dist/

      - name: Installation de Databricks CLI
        run: pip install databricks-cli databricks-sdk

      - name: V√©rifier Databricks CLI et acc√®s
        run: |
          echo "--- databricks binary ---"
          databricks --version || true
          which databricks || true
          echo "--- pip packages ---"
          pip show databricks-cli databricks-sdk || true
          echo "--- tenter lister la racine Volumes (debug) ---"
          # Liste racine DBFS (peut √©chouer si path non support√©)
          databricks fs ls dbfs:/Volumes || databricks fs ls dbfs:/ || true

      - name: Configuration de Databricks CLI (Service Principal)
        env:
          ARM_CLIENT_ID: ${{ secrets.ARM_CLIENT_ID }}
          ARM_CLIENT_SECRET: ${{ secrets.ARM_CLIENT_SECRET }}
          ARM_TENANT_ID: ${{ secrets.ARM_TENANT_ID }}
          DATABRICKS_HOST: ${{ vars.DATABRICKS_HOST }}
          DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
        run: |
          set -euo pipefail
          echo "[DEFAULT]" > ~/.databrickscfg
          echo "host = ${DATABRICKS_HOST}" >> ~/.databrickscfg

          if [ -n "${DATABRICKS_TOKEN:-}" ]; then
            echo "Using DATABRICKS_TOKEN secret for authentication"
            echo "token = ${DATABRICKS_TOKEN}" >> ~/.databrickscfg
          else
            echo "No DATABRICKS_TOKEN secret found ‚Äî attempting Azure AD client credentials flow"
            # Login with service principal
            az login --service-principal -u "$ARM_CLIENT_ID" -p "$ARM_CLIENT_SECRET" --tenant "$ARM_TENANT_ID" >/dev/null

            # Request an access token for the Databricks resource
            TOKEN=$(az account get-access-token --resource 2ff814a6-3304-4ab8-85cb-cd0e6f879c1d --query accessToken -o tsv || true)
            if [ -z "$TOKEN" ]; then
              echo "Failed to obtain AAD token for Databricks via az. Ensure the service principal has the necessary permissions and the Databricks resource ID is allowed."
              exit 1
            fi

            echo "token = $TOKEN" >> ~/.databrickscfg
          fi

          # Quick validation: try listing the target volume path (may return 403 if SP lacks workspace privileges)
          echo "--- Validate Databricks auth by listing target volume ---"
          databricks fs ls "dbfs:/Volumes" || databricks fs ls "dbfs:/" || true

          echo "‚úÖ Authentification configur√©e"

      - name: D√©finition des variables de d√©ploiement
        id: vars
        run: |
          # Catalog Bronze pour les artifacts
          BRONZE_CATALOG="${{ env.CATALOG_PREFIX }}_${{ env.ENV_NAME }}_bronze"
          
          # Chemin du volume pour les packages
          VOLUME_PATH="/Volumes/${BRONZE_CATALOG}/artifacts/packages/${{ env.DEVELOPER_NAME }}"
          
          echo "BRONZE_CATALOG=$BRONZE_CATALOG" >> $GITHUB_OUTPUT
          echo "VOLUME_PATH=$VOLUME_PATH" >> $GITHUB_OUTPUT

      - name: Affichage de l'environnement de d√©ploiement
        run: |
          echo "=========================================="
          echo "üöÄ CONFIGURATION DU D√âPLOIEMENT"
          echo "=========================================="
          echo "üë§ D√©clench√© par: ${{ github.actor }}"
          echo "üåø Branche: ${{ github.ref_name }}"
          echo "üè∑Ô∏è  D√©veloppeur: ${{ env.DEVELOPER_NAME }}"
          echo "üåç Environnement: ${{ env.ENV_NAME }}"
          echo ""
          echo "üì¶ Databricks Host: ${{ env.DATABRICKS_HOST }}"
          echo "üîê Auth: Service Principal"
          echo "ü•â Bronze Catalog: ${{ steps.vars.outputs.BRONZE_CATALOG }}"
          echo "üìÅ Volume Path: ${{ steps.vars.outputs.VOLUME_PATH }}"
          echo ""
          echo "üìã Packages √† d√©ployer:"
          ls -lh dist/
          echo "=========================================="

      - name: Upload du wheel vers Unity Catalog Volume
        run: |
          set -x
          WHEEL_FILE=$(ls dist/*.whl | head -1)
          WHEEL_NAME=$(basename "$WHEEL_FILE")
          VOLUME_PATH="${{ steps.vars.outputs.VOLUME_PATH }}"
          echo "üì§ Upload de $WHEEL_NAME"
          echo "   Destination: $VOLUME_PATH/$WHEEL_NAME"
          # Capture stdout/stderr pour d√©bogage (JSONDecodeError provient souvent d'une sortie non-JSON)
          databricks fs cp "$WHEEL_FILE" "dbfs:$VOLUME_PATH/$WHEEL_NAME" --overwrite 2>&1 | tee /tmp/databricks-upload.log
          STATUS=${PIPESTATUS[0]}
          echo "Exit code: $STATUS"
          echo "--- upload log ---"
          sed -n '1,200p' /tmp/databricks-upload.log || true
          echo "--- lister le contenu du volume cible ---"
          databricks fs ls "dbfs:$VOLUME_PATH" 2>&1 | tee /tmp/databricks-ls.log || true
          echo "--- ls log ---"
          sed -n '1,200p' /tmp/databricks-ls.log || true
          if [ $STATUS -ne 0 ]; then
            echo "Erreur lors de l'upload du package (voir /tmp/databricks-upload.log)"
            exit $STATUS
          fi
          echo ""
          echo "‚úÖ Package upload√© avec succ√®s!"
          echo ""
          echo "üìù Pour installer ce package dans un notebook Databricks:"
          echo "   %pip install $VOLUME_PATH/$WHEEL_NAME"

      - name: V√©rification du d√©ploiement
        run: |
          VOLUME_PATH="${{ steps.vars.outputs.VOLUME_PATH }}"
          echo "üìã Contenu du volume:"
          databricks fs ls "dbfs:$VOLUME_PATH"
        continue-on-error: true

      - name: R√©sum√© du d√©ploiement
        run: |
          echo "=========================================="
          echo "‚úÖ D√âPLOIEMENT TERMIN√â"
          echo "=========================================="
          echo "Environnement: ${{ env.ENV_NAME }}"
          echo "D√©veloppeur: ${{ env.DEVELOPER_NAME }}"
          echo "Catalog: ${{ steps.vars.outputs.BRONZE_CATALOG }}"
          echo "Volume: ${{ steps.vars.outputs.VOLUME_PATH }}"
          echo "Branche: ${{ github.ref_name }}"
          echo "Commit: ${{ github.sha }}"
          echo "Auth: Service Principal (${{ env.ARM_CLIENT_ID }})"
          echo "D√©clench√© par: ${{ github.actor }}"
          echo "=========================================="
#         id: vars
#         run: |
#           # Catalog Bronze pour les artifacts
#           BRONZE_CATALOG="${{ env.CATALOG_PREFIX }}_${{ env.ENV_NAME }}_bronze"
          
#           # Chemin du volume pour les packages
#           VOLUME_PATH="/Volumes/${BRONZE_CATALOG}/artifacts/packages/${{ env.DEVELOPER_NAME }}"
          
#           echo "BRONZE_CATALOG=$BRONZE_CATALOG" >> $GITHUB_OUTPUT
#           echo "VOLUME_PATH=$VOLUME_PATH" >> $GITHUB_OUTPUT

#       - name: Affichage de l'environnement de d√©ploiement
#         run: |
#           echo "=========================================="
#           echo "üöÄ CONFIGURATION DU D√âPLOIEMENT"
#           echo "=========================================="
#           echo "üë§ D√©clench√© par: ${{ github.actor }}"
#           echo "üåø Branche: ${{ github.ref_name }}"
#           echo "üè∑Ô∏è  D√©veloppeur: ${{ env.DEVELOPER_NAME }}"
#           echo "üåç Environnement: ${{ env.ENV_NAME }}"
#           echo ""
#           echo "üì¶ Databricks Host: ${{ env.DATABRICKS_HOST }}"
#           echo "üîê Auth: Service Principal"
#           echo "ü•â Bronze Catalog: ${{ steps.vars.outputs.BRONZE_CATALOG }}"
#           echo "üìÅ Volume Path: ${{ steps.vars.outputs.VOLUME_PATH }}"
#           echo ""
#           echo "üìã Packages √† d√©ployer:"
#           ls -lh dist/
#           echo "=========================================="

#       - name: Upload du wheel vers Unity Catalog Volume
#         run: |
#           WHEEL_FILE=$(ls dist/*.whl | head -1)
#           WHEEL_NAME=$(basename $WHEEL_FILE)
#           VOLUME_PATH="${{ steps.vars.outputs.VOLUME_PATH }}"
          
#           echo "üì§ Upload de $WHEEL_NAME"
#           echo "   Destination: $VOLUME_PATH/$WHEEL_NAME"
          
#           databricks fs cp "$WHEEL_FILE" "dbfs:$VOLUME_PATH/$WHEEL_NAME" --overwrite
          
#           echo ""
#           echo "‚úÖ Package upload√© avec succ√®s!"
#           echo ""
#           echo "üìù Pour installer ce package dans un notebook Databricks:"
#           echo "   %pip install $VOLUME_PATH/$WHEEL_NAME"

#       - name: V√©rification du d√©ploiement
#         run: |
#           VOLUME_PATH="${{ steps.vars.outputs.VOLUME_PATH }}"
#           echo "üìã Contenu du volume:"
#           databricks fs ls "dbfs:$VOLUME_PATH"
#         continue-on-error: true

#       - name: R√©sum√© du d√©ploiement
#         run: |
#           echo "=========================================="
#           echo "‚úÖ D√âPLOIEMENT TERMIN√â"
#           echo "=========================================="
#           echo "Environnement: ${{ env.ENV_NAME }}"
#           echo "D√©veloppeur: ${{ env.DEVELOPER_NAME }}"
#           echo "Catalog: ${{ steps.vars.outputs.BRONZE_CATALOG }}"
#           echo "Volume: ${{ steps.vars.outputs.VOLUME_PATH }}"
#           echo "Branche: ${{ github.ref_name }}"
#           echo "Commit: ${{ github.sha }}"
#           echo "Auth: Service Principal (${{ env.ARM_CLIENT_ID }})"
#           echo "D√©clench√© par: ${{ github.actor }}"
#           echo "=========================================="