name: CD - DÃ©ploiement Databricks

on:
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environnement de dÃ©ploiement'
        required: true
        default: 'dev'
        type: choice
        options: [dev, test, prod]

      developer_name:
        description: 'Nom du dÃ©veloppeur (ex: godwin)'
        required: false
        default: 'dev'
        type: string

      skip_tests:
        description: 'Ignorer les tests (uniquement pour dev)'
        required: false
        default: false
        type: boolean

env:
  PYTHON_VERSION: '3.11'
  CATALOG_PREFIX: 'daie_chn'

permissions:
  contents: read

# ============================================================
# TESTS
# ============================================================
jobs:
  test:
    name: Tests avant dÃ©ploiement
    runs-on: ubuntu-latest
    if: ${{ !(github.event.inputs.environment == 'dev' && github.event.inputs.skip_tests == 'true') }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: pip

      - name: Setup Java
        uses: actions/setup-java@v4
        with:
          distribution: temurin
          java-version: '11'

      - name: Install test dependencies
        run: |
          pip install -U pip
          pip install -r test-requirements.txt
          pip install -e . --no-deps

      - name: Run tests
        run: pytest tests/ -v --cov=src --cov-report=term-missing

# ============================================================
# BUILD
# ============================================================
  build:
    name: Build Python package
    runs-on: ubuntu-latest
    needs: test
    if: always() && (needs.test.result == 'success' || needs.test.result == 'skipped')

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: pip

      - name: Build wheel package
        run: |
          pip install -U pip build wheel
          python -m build
          
          echo ""
          echo "ğŸ“¦ Package built:"
          ls -lh dist/

      - name: Upload wheel artifact
        uses: actions/upload-artifact@v4
        with:
          name: python-wheel
          path: dist/*.whl
          retention-days: 7
          if-no-files-found: error

# ============================================================
# DEPLOY
# ============================================================
  deploy:
    name: DÃ©ploiement sur Databricks
    runs-on: ubuntu-latest
    needs: build
    environment: ${{ github.event.inputs.environment }}

    env:
      ENV_NAME: ${{ github.event.inputs.environment }}
      DEVELOPER_NAME: ${{ github.event.inputs.developer_name }}
      DATABRICKS_HOST: ${{ vars.DATABRICKS_HOST }}
      
      # Authentification Azure AD Service Principal
      ARM_CLIENT_ID: ${{ secrets.ARM_CLIENT_ID }}
      ARM_CLIENT_SECRET: ${{ secrets.ARM_CLIENT_SECRET }}
      ARM_TENANT_ID: ${{ secrets.ARM_TENANT_ID }}
      ARM_SUBSCRIPTION_ID: ${{ secrets.ARM_SUBSCRIPTION_ID }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Download wheel artifact
        uses: actions/download-artifact@v4
        with:
          name: python-wheel
          path: dist/

      # --------------------------------------------------------
      # Install Databricks CLI
      # --------------------------------------------------------
      - name: Install Databricks CLI
        run: |
          pip install -U pip
          pip install databricks-cli databricks-sdk
          
          # VÃ©rifier l'installation
          databricks --version
          echo ""
          echo "âœ… Databricks CLI installed"

      # --------------------------------------------------------
      # Configure Databricks authentication
      # --------------------------------------------------------
      - name: Configure Databricks authentication
        run: |
          # VÃ©rifier les variables d'environnement requises
          if [[ -z "$DATABRICKS_HOST" ]]; then
            echo "âŒ DATABRICKS_HOST is not set"
            exit 1
          fi
          
          if [[ -z "$ARM_CLIENT_ID" ]]; then
            echo "âŒ ARM_CLIENT_ID is not set"
            exit 1
          fi
          
          # Nettoyer l'URL (enlever le https:// si prÃ©sent)
          DATABRICKS_HOST_CLEAN=$(echo "$DATABRICKS_HOST" | sed 's|https://||')
          
          # CrÃ©er le fichier de configuration Databricks
          cat > ~/.databrickscfg <<EOF
          [DEFAULT]
          host = https://${DATABRICKS_HOST_CLEAN}
          azure_client_id = ${ARM_CLIENT_ID}
          azure_client_secret = ${ARM_CLIENT_SECRET}
          azure_tenant_id = ${ARM_TENANT_ID}
          EOF
          
          chmod 600 ~/.databrickscfg
          
          echo "âœ… Databricks config created"
          echo "ğŸ“‹ Host: https://${DATABRICKS_HOST_CLEAN}"

      # --------------------------------------------------------
      # Test Databricks connection
      # --------------------------------------------------------
      - name: Test Databricks connection
        run: |
          echo "ğŸ” Testing connection to Databricks workspace..."
          
          # Tester la connexion
          if databricks workspace list / --profile DEFAULT > /dev/null 2>&1; then
            echo "âœ… Connection successful"
          else
            echo "âš ï¸  Workspace list failed, but continuing (might be permissions)"
          fi
          
          # Afficher les informations de connexion (sans les secrets)
          echo ""
          echo "ğŸ“‹ Connection info:"
          echo "   Host: $DATABRICKS_HOST"
          echo "   Tenant: $ARM_TENANT_ID"

      # --------------------------------------------------------
      # Compute deployment variables
      # --------------------------------------------------------
      - name: Compute deployment variables
        id: vars
        run: |
          # DÃ©finir les variables de dÃ©ploiement
          BRONZE_CATALOG="${CATALOG_PREFIX}_${ENV_NAME}_bronze"
          VOLUME_BASE="/Volumes/${BRONZE_CATALOG}/artifacts/packages"
          VOLUME_PATH="${VOLUME_BASE}/${DEVELOPER_NAME}"
          
          # RÃ©cupÃ©rer le nom du wheel
          WHEEL_FILE=$(ls dist/*.whl | head -1)
          WHEEL_NAME=$(basename "$WHEEL_FILE")
          
          # Exporter les variables
          echo "BRONZE_CATALOG=${BRONZE_CATALOG}" >> $GITHUB_OUTPUT
          echo "VOLUME_PATH=${VOLUME_PATH}" >> $GITHUB_OUTPUT
          echo "WHEEL_FILE=${WHEEL_FILE}" >> $GITHUB_OUTPUT
          echo "WHEEL_NAME=${WHEEL_NAME}" >> $GITHUB_OUTPUT
          
          # Afficher les informations
          echo ""
          echo "ğŸ“¦ Deployment variables:"
          echo "   Catalog: ${BRONZE_CATALOG}"
          echo "   Volume: ${VOLUME_PATH}"
          echo "   Package: ${WHEEL_NAME}"

      # --------------------------------------------------------
      # Create Volume directory structure
      # --------------------------------------------------------
      - name: Create Volume directory
        run: |
          VOLUME_PATH="${{ steps.vars.outputs.VOLUME_PATH }}"
          
          echo "ğŸ“ Creating volume directory: ${VOLUME_PATH}"
          
          # CrÃ©er le rÃ©pertoire dans le volume Unity Catalog
          # Note: la commande peut Ã©chouer si le rÃ©pertoire existe dÃ©jÃ 
          databricks fs mkdirs "dbfs:${VOLUME_PATH}" --profile DEFAULT || {
            echo "âš ï¸  Directory might already exist, continuing..."
          }

      # --------------------------------------------------------
      # Upload wheel to Unity Catalog Volume
      # --------------------------------------------------------
      - name: Upload wheel to Unity Catalog Volume
        run: |
          set -e
          
          WHEEL_FILE="${{ steps.vars.outputs.WHEEL_FILE }}"
          VOLUME_PATH="${{ steps.vars.outputs.VOLUME_PATH }}"
          WHEEL_NAME="${{ steps.vars.outputs.WHEEL_NAME }}"
          
          echo ""
          echo "ğŸ“¤ Uploading wheel package..."
          echo "   Source: ${WHEEL_FILE}"
          echo "   Destination: dbfs:${VOLUME_PATH}/${WHEEL_NAME}"
          echo ""
          
          # Upload du fichier wheel avec overwrite
          databricks fs cp "${WHEEL_FILE}" "dbfs:${VOLUME_PATH}/${WHEEL_NAME}" \
            --overwrite \
            --profile DEFAULT
          
          echo ""
          echo "âœ… Upload successful"

      # --------------------------------------------------------
      # Verify upload
      # --------------------------------------------------------
      - name: Verify upload
        run: |
          VOLUME_PATH="${{ steps.vars.outputs.VOLUME_PATH }}"
          WHEEL_NAME="${{ steps.vars.outputs.WHEEL_NAME }}"
          
          echo ""
          echo "ğŸ” Verifying upload..."
          echo ""
          
          # Lister le contenu du volume
          if databricks fs ls "dbfs:${VOLUME_PATH}/" --profile DEFAULT; then
            echo ""
            echo "âœ… Package verified in volume"
          else
            echo ""
            echo "âŒ Failed to verify package"
            exit 1
          fi

      # --------------------------------------------------------
      # Deployment summary
      # --------------------------------------------------------
      - name: Deployment summary
        if: always()
        run: |
          WHEEL_NAME="${{ steps.vars.outputs.WHEEL_NAME }}"
          BRONZE_CATALOG="${{ steps.vars.outputs.BRONZE_CATALOG }}"
          VOLUME_PATH="${{ steps.vars.outputs.VOLUME_PATH }}"
          
          echo ""
          echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
          echo "â•‘        âœ… DÃ‰PLOIEMENT DATABRICKS TERMINÃ‰               â•‘"
          echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo ""
          echo "ğŸ“‹ Informations de dÃ©ploiement:"
          echo "   â€¢ Environment    : ${{ env.ENV_NAME }}"
          echo "   â€¢ Developer      : ${{ env.DEVELOPER_NAME }}"
          echo "   â€¢ Catalog        : ${BRONZE_CATALOG}"
          echo "   â€¢ Volume         : ${VOLUME_PATH}"
          echo "   â€¢ Package        : ${WHEEL_NAME}"
          echo ""
          echo "ğŸ”— Databricks:"
          echo "   â€¢ Workspace      : ${{ env.DATABRICKS_HOST }}"
          echo ""
          echo "ğŸ“ Git:"
          echo "   â€¢ Commit         : ${{ github.sha }}"
          echo "   â€¢ Branch         : ${{ github.ref_name }}"
          echo "   â€¢ Actor          : ${{ github.actor }}"
          echo ""
          echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
          echo "â•‘  ğŸ“¦ INSTALLATION DANS UN NOTEBOOK DATABRICKS          â•‘"
          echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo ""
          echo "%pip install /Volumes/${BRONZE_CATALOG}/artifacts/packages/${{ env.DEVELOPER_NAME }}/${WHEEL_NAME}"
          echo ""
          echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo ""

      # --------------------------------------------------------
      # Notification (optionnel)
      # --------------------------------------------------------
      - name: Send notification
        if: failure()
        run: |
          echo "âŒ Deployment failed!"
          echo "Check the logs above for more details."